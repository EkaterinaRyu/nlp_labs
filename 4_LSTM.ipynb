{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1EQqJABGEIW_kwOLYdrRCGSfMOJ94B97P","authorship_tag":"ABX9TyPVtMfaRnZX+D/b5WuFB1vl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","from collections import Counter\n","import os\n","from argparse import Namespace\n","import collections"],"metadata":{"id":"ITemUTmxIYu7","executionInfo":{"status":"ok","timestamp":1701440427066,"user_tz":-180,"elapsed":3170,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oyi7bt63neQj","executionInfo":{"status":"ok","timestamp":1701440440626,"user_tz":-180,"elapsed":1901,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"bcf4ed77-5830-40a2-b8c3-9c12375aaa6e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["##Лаб_4. Генерация текстов на основе LSTM\n"],"metadata":{"id":"4lAm_ikHAWvt"}},{"cell_type":"markdown","source":["Задание 1. Загрузите текст из произведений Ницше ('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt').\n","Выведете следующее:\n","* А) длину всего корпуса;\n","* Б) количество предложений;\n","* В) сколько всего символов используется?\n"],"metadata":{"id":"L3c52FAgAiJq"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCT4ZM2JY3lB","executionInfo":{"status":"ok","timestamp":1701440446339,"user_tz":-180,"elapsed":1467,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"c90788f0-7ebe-49ae-e826-ece9923d2159"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{},"execution_count":3}],"source":["f = open(r'/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/nietzsche.txt', 'r').read().lower()\n","type(f)"]},{"cell_type":"code","source":["print(f\"Symbols: {len(f)}\\nUnique symbols: {len(set(f))}\",\n","      f\"\\n\\nWords: {len(f.split(' '))}\\nTokenized words: {len(word_tokenize(f))}\",\n","      f\"\\nUnique words: {len(set(f.split(' ')))}\\nUnique tokenized words: {len(set(word_tokenize(f)))}\",\n","      f\"\\n\\nSentences : {len(f.split('.'))}\\nTokenized sentences: {len(sent_tokenize(f))}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQBpbhKMLMGN","executionInfo":{"status":"ok","timestamp":1701408819298,"user_tz":-180,"elapsed":1224,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"ebef97c4-17a3-4079-831b-c00765c703fe"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Symbols: 600893\n","Unique symbols: 57 \n","\n","Words: 91209\n","Tokenized words: 117653 \n","Unique words: 22183\n","Unique tokenized words: 11012 \n","\n","Sentences : 2788\n","Tokenized sentences: 2864\n"]}]},{"cell_type":"code","source":["class Vocab:\n","    def __init__(self, data):\n","        self.data = data\n","        self.idx_to_token = dict(enumerate(self.data))\n","        self.token_to_idx = {char : idx for idx, char in self.idx_to_token.items()}\n","        self.vocab_len = len(self.token_to_idx)\n","\n","vocab_words = Vocab(set(word_tokenize(f)))\n","vocab_chars = Vocab(set(f))"],"metadata":{"id":"KSLM5i2a72aF","executionInfo":{"status":"ok","timestamp":1701440450685,"user_tz":-180,"elapsed":943,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(vocab_chars.idx_to_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Nst5bnn8ECE","executionInfo":{"status":"ok","timestamp":1701408825139,"user_tz":-180,"elapsed":313,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"9ab8ce17-02ab-4d6a-cef6-21465566c959"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'j', 1: 'a', 2: 'c', 3: 'b', 4: 'm', 5: '8', 6: ',', 7: '-', 8: '0', 9: '.', 10: 'r', 11: '3', 12: 'k', 13: '1', 14: 'o', 15: ']', 16: 'ë', 17: ':', 18: 'l', 19: '6', 20: '(', 21: '[', 22: '?', 23: '!', 24: 'æ', 25: ' ', 26: 'y', 27: 'i', 28: 'e', 29: 'v', 30: 't', 31: 'u', 32: '2', 33: 'd', 34: 'é', 35: 'z', 36: ')', 37: '\"', 38: '\\n', 39: 'g', 40: ';', 41: 'w', 42: 'x', 43: 'ä', 44: '=', 45: '4', 46: 's', 47: 'h', 48: 'q', 49: '9', 50: \"'\", 51: '5', 52: 'f', 53: 'p', 54: '_', 55: '7', 56: 'n'}\n"]}]},{"cell_type":"markdown","source":["Задание 2. Сократите текст наполовину избыточными последовательностями символов maxlen\n","* определить словарь vocab\n","* разбить на проследовательности по 40 токенов с шагом 3\n","* векторизовать последовательности\n","\n"],"metadata":{"id":"z_tgBdFyAkrU"}},{"cell_type":"code","source":["\"\"\"\n","по буквам - передавать строку\n","по словам - передавать список слов\n","\"\"\"\n","\n","def sequence(text, vocab, maxlen = 40, step = 3):\n","    sentences = []\n","    next_tokens = []\n","    for i in range(0, len(text) - maxlen, step):\n","        sentences.append(text[i: i + maxlen])\n","        next_tokens.append(text[i + maxlen])\n","\n","    \"\"\"\n","    для каждого предложения, для каждого слова в предложении\n","    x : в положении [индекс предложения, индекс токена в предложении, индекс этого же токена из словаря] = 1\n","    y : в положении [индекс предложения, индекс следующего токена из словаря] = 1\n","    \"\"\"\n","\n","    print('Vectorization...')\n","    x = np.zeros((len(sentences), maxlen, len(vocab)))\n","    y = np.zeros((len(sentences), len(vocab)))\n","    for s, sent in enumerate(sentences):\n","        for t, token in enumerate(sent):\n","            x[s, t, vocab[token]] = 1\n","        y[s, vocab[next_tokens[s]]] = 1\n","    return x, y"],"metadata":{"id":"4Sy0qNKnAwgm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y = sequence(word_tokenize(f), vocab_words.token_to_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxEFuM6nwihS","executionInfo":{"status":"ok","timestamp":1700846973865,"user_tz":-180,"elapsed":6407,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"a2c11635-7e5e-43b2-9e3f-b640fb1512f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vectorization...\n"]}]},{"cell_type":"code","source":["x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNlWlOXHYZ1_","executionInfo":{"status":"ok","timestamp":1700847013213,"user_tz":-180,"elapsed":416,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"6bef884d-edf0-49db-e22a-8cc085c55654"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(39205, 40, 11012)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Задание 3. Создайте модель LSTM для генерации текста\n","* А) Напишите вспомогательную функцию для выборки индекса из массива вероятностей\n","* Б) Напишите функцию, которая будет вызываться в конце каждой эпохи и печатать сгенерированный текст\n","* В) Запустите модель на обучение\n","Имейте ввиду, что требуется не менее 20 эпох, прежде чем сгенерированный текст начнет звучать связно. Рекомендуется запускать этот скрипт на графическом процессоре, так как рекуррентные сети требуют довольно больших вычислительных затрат.\n","* Г) Проверьте работу модели в онлайн режиме.\n","Для примера смотрите файл lstm_генерация.ipynb\n"],"metadata":{"id":"4iMtNfsCA7Ab"}},{"cell_type":"code","source":[],"metadata":{"id":"0H3yVHLdBDer"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Задание 4. Создайте самостоятельно генерацию текста для РУССКОЯЗЫЧНОГО НАБОРА глав Wikibooks.\n","Полный текст Wikibooks содержит более 270000 глав на 12 языках https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset/data\n"],"metadata":{"id":"owJCqnqwBPub"}},{"cell_type":"code","source":["is_cuda = False\n","\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LhTBYnfBS-h","executionInfo":{"status":"ok","timestamp":1701440456813,"user_tz":-180,"elapsed":436,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"2f377293-132a-4bc0-d190-123dc2d14d2d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU not available, CPU used\n"]}]},{"cell_type":"code","source":["Flags = collections.namedtuple('Flags', 'text seq_size batch_size embedding_size lstm_size gradients_norm initial_words predict_top_k checkpoint_path learning_rate')\n","# print with_class._fields\n","\n","flags = Flags(\n","    text=f,\n","    seq_size=32,\n","    batch_size=16,\n","    embedding_size=64,\n","    lstm_size=64,\n","    gradients_norm=5,\n","    initial_words=['woman', 'do'],\n","    predict_top_k=5,\n","    checkpoint_path='checkpoint',\n","    learning_rate = 0.01\n",")"],"metadata":{"id":"2XNmQi4tIpL5","executionInfo":{"status":"ok","timestamp":1701440459857,"user_tz":-180,"elapsed":6,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def get_data_from_file(text, batch_size, seq_size):\n","    text = text.split()\n","\n","    word_counts = Counter(text)\n","    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n","    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n","    n_vocab = len(int_to_vocab)\n","\n","    print('Vocabulary size ', n_vocab)\n","\n","    int_text = [vocab_to_int[w] for w in text]\n","    num_batches = int(len(int_text) / (seq_size * batch_size))\n","\n","    print(f'num_batches: {num_batches}')\n","\n","    in_text = int_text[:num_batches * batch_size * seq_size]\n","    out_text = np.zeros_like(in_text)\n","    out_text[:-1] = in_text[1:]\n","    out_text[-1] = in_text[0]\n","    in_text = np.reshape(in_text, (batch_size, -1))\n","    out_text = np.reshape(out_text, (batch_size, -1))\n","\n","    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"],"metadata":{"id":"S40Kq9SLIq8L","executionInfo":{"status":"ok","timestamp":1701440465269,"user_tz":-180,"elapsed":8,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def get_batches(in_text, out_text, batch_size, seq_size):\n","    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n","    for i in range(0, num_batches * seq_size, seq_size):\n","        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"],"metadata":{"id":"JFo3F80RIq1r","executionInfo":{"status":"ok","timestamp":1701440468648,"user_tz":-180,"elapsed":393,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class RNNModule(nn.Module):\n","    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n","        super(RNNModule, self).__init__()\n","        self.seq_size = seq_size\n","        self.lstm_size = lstm_size\n","\n","        self.embedding = nn.Embedding(n_vocab, embedding_size)\n","        self.lstm = nn.LSTM(embedding_size,\n","                            lstm_size,\n","                            batch_first=True)\n","        self.dense = nn.Linear(lstm_size, n_vocab)\n","\n","    def forward(self, x, prev_state):\n","        embed = self.embedding(x)\n","        output, state = self.lstm(embed, prev_state)\n","        logits = self.dense(output)\n","        return logits, state\n","\n","    def zero_state(self, batch_size):\n","        return (torch.zeros(1, batch_size, self.lstm_size),\n","                torch.zeros(1, batch_size, self.lstm_size))"],"metadata":{"id":"nzERUQ_mIqoJ","executionInfo":{"status":"ok","timestamp":1701440471231,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def predict(device, model, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n","    model.eval()\n","\n","    state_h, state_c = model.zero_state(1)\n","    state_h = state_h.to(device)\n","    state_c = state_c.to(device)\n","\n","    for w in (w for wl in words for w in wl):\n","        ix = torch.tensor([[vocab_to_int[w]]], dtype=torch.long).to(device)\n","        output, (state_h, state_c) = model(ix, (state_h, state_c))\n","\n","    _, top_ix = torch.topk(output[0], k=top_k)\n","    choices = top_ix.tolist()\n","    choice = np.random.choice(choices[0])\n","\n","    words_new = list()\n","    words_new.append(int_to_vocab[choice])\n","\n","    for _ in range(50):\n","        ix = torch.tensor([[choice]], dtype=torch.long).to(device)\n","        output, (state_h, state_c) = model(ix, (state_h, state_c))\n","\n","        _, top_ix = torch.topk(output[0], k=top_k) # A namedtuple of (values, indices) is returned\n","        choices = top_ix.tolist()\n","        choice = np.random.choice(choices[0])\n","        words_new.append(int_to_vocab[choice])\n","\n","    words.append(words_new)\n","\n","    print('\\n\\n'.join(' '.join(s) for s in words))"],"metadata":{"id":"Gt-VbVr-I0Iy","executionInfo":{"status":"ok","timestamp":1701440474687,"user_tz":-180,"elapsed":450,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n","    flags.text, flags.batch_size, flags.seq_size)\n","\n","iteration = 20000\n","\n","model = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n","model = model.to(device)\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/checkpoint_pt/model-20000.pth\"))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1ScTawSI0FT","executionInfo":{"status":"ok","timestamp":1701440615363,"user_tz":-180,"elapsed":3564,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"3c0089d7-4461-425a-ca45-bcaaccf2e20e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size  17682\n","num_batches: 193\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","n_epochs = 3\n","\n","for e in tqdm(range(n_epochs)):\n","    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n","    state_h, state_c = model.zero_state(flags.batch_size)\n","\n","    state_h = state_h.to(device)\n","    state_c = state_c.to(device)\n","\n","    for x, y in batches:\n","#         print(type(x), x.shape, x)\n","        iteration += 1\n","\n","        model.train() # Tell it we are in training mode\n","\n","        optimizer.zero_grad() # Reset all gradients\n","\n","        x = torch.tensor(x, dtype=torch.long).to(device)\n","        y = torch.tensor(y, dtype=torch.long).to(device)\n","\n","        logits, (state_h, state_c) = model(x, (state_h, state_c))\n","        loss = criterion(logits.transpose(1, 2), y)\n","\n","        state_h = state_h.detach()\n","        state_c = state_c.detach()\n","\n","        loss_value = loss.item()\n","\n","        loss.backward() # Perform back-propagation\n","\n","        _ = torch.nn.utils.clip_grad_norm_(\n","            model.parameters(), flags.gradients_norm)\n","\n","        optimizer.step()\n","\n","        if iteration % 50 == 0:\n","            print(f'Epoch: {e}/{n_epochs}\\nIteration: {iteration}\\nLoss: {loss_value}')\n","\n","        if iteration % 1000 == 0:\n","            predict(device, model, [flags.initial_words], n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n","\n","            torch.save(model.state_dict(),\n","                       f'/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/checkpoint_pt/model-{iteration}.pth')"],"metadata":{"id":"0Vd8ko2_I9Sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UCLYQuWuI9PK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["karenina = open(r\"/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/AnnaKarenina__.txt\", \"r\").read().lower()"],"metadata":{"id":"8dIcPFpC7CnL","executionInfo":{"status":"ok","timestamp":1701409895338,"user_tz":-180,"elapsed":819,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["Flags = collections.namedtuple('Flags', 'text seq_size batch_size embedding_size lstm_size gradients_norm initial_words predict_top_k checkpoint_path learning_rate')\n","flags2 = Flags(\n","    text=karenina,\n","    seq_size=32,\n","    batch_size=16,\n","    embedding_size=64,\n","    lstm_size=64,\n","    gradients_norm=5,\n","    initial_words=['я', 'очень'],\n","    predict_top_k=5,\n","    checkpoint_path='checkpoint',\n","    learning_rate = 0.01\n",")"],"metadata":{"id":"FwPfxAZmI9M1","executionInfo":{"status":"ok","timestamp":1701409897885,"user_tz":-180,"elapsed":295,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n","    flags2.text, flags2.batch_size, flags2.seq_size)\n","\n","model = RNNModule(n_vocab, flags2.seq_size, flags2.embedding_size, flags2.lstm_size)\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=flags2.learning_rate)\n","\n","iteration = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMTPh-QXI9KN","executionInfo":{"status":"ok","timestamp":1700856624765,"user_tz":-180,"elapsed":1177,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"1fccca0f-1705-4189-eb6a-9f4cc4bd8605"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size  53301\n","num_batches: 576\n"]}]},{"cell_type":"code","source":["n_epochs = 200\n","\n","for e in range(n_epochs):\n","    batches = get_batches(in_text, out_text, flags2.batch_size, flags2.seq_size)\n","    state_h, state_c = model.zero_state(flags2.batch_size)\n","\n","    state_h = state_h.to(device)\n","    state_c = state_c.to(device)\n","\n","    for x, y in batches:\n","#         print(type(x), x.shape, x)\n","        iteration += 1\n","\n","        model.train() # Tell it we are in training mode\n","\n","        optimizer.zero_grad() # Reset all gradients\n","\n","        x = torch.tensor(x, dtype=torch.long).to(device)\n","        y = torch.tensor(y, dtype=torch.long).to(device)\n","\n","        logits, (state_h, state_c) = model(x, (state_h, state_c))\n","\n","        loss = criterion(logits.transpose(1, 2), y)\n","\n","        state_h = state_h.detach()\n","        state_c = state_c.detach()\n","\n","        loss_value = loss.item()\n","\n","        loss.backward() # Perform back-propagation\n","\n","        # gradient clipping:\n","        _ = torch.nn.utils.clip_grad_norm_(\n","            model.parameters(), flags2.gradients_norm)\n","\n","        optimizer.step()\n","\n","        if iteration % 50 == 0:\n","            print(f'Epoch: {e}/{n_epochs} Iteration: {iteration} Loss: {loss_value}')\n","\n","        if iteration % 1000 == 0:\n","            predict(device, model, [flags2.initial_words], n_vocab, vocab_to_int, int_to_vocab)\n","\n","            torch.save(model.state_dict(),\n","                       f'/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/checkpoint_pt/model_ru-{iteration}.pth')"],"metadata":{"id":"8Jb0rlBA7jVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n","    flags2.text, flags2.batch_size, flags2.seq_size)\n","\n","iteration = 6000\n","\n","model = RNNModule(n_vocab, flags2.seq_size, flags2.embedding_size, flags2.lstm_size)\n","model = model.to(device)\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/checkpoint_pt/model_ru-{iteration}.pth\"))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=flags2.learning_rate)\n","\n","predict(device, model, [['помоги', 'мне']], n_vocab, vocab_to_int, int_to_vocab)"],"metadata":{"id":"qGi8rypb9Swe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701409933271,"user_tz":-180,"elapsed":1779,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"8f7b87c9-7511-4687-9657-5290ef05f501"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size  53301\n","num_batches: 576\n","помоги мне\n","\n","только и я скажу ей?» и я знаю, не буду, что вы знаете, – сказала княгиня. и опять туда показалось, – что я могу отдать его чем-нибудь?» и я кое-как сказать… я делаюсь воли наступило мертвое, в 70-е годы: обнятою огромной руки. она была не одна, возможна и сестрой к нему\n"]}]},{"cell_type":"code","source":["n_epochs = 20\n","\n","for e in range(n_epochs):\n","    batches = get_batches(in_text, out_text, flags2.batch_size, flags2.seq_size)\n","    state_h, state_c = model.zero_state(flags2.batch_size)\n","\n","    state_h = state_h.to(device)\n","    state_c = state_c.to(device)\n","\n","    for x, y in batches:\n","#         print(type(x), x.shape, x)\n","        iteration += 1\n","\n","        model.train() # Tell it we are in training mode\n","\n","        optimizer.zero_grad() # Reset all gradients\n","\n","        x = torch.tensor(x, dtype=torch.long).to(device)\n","        y = torch.tensor(y, dtype=torch.long).to(device)\n","\n","        logits, (state_h, state_c) = model(x, (state_h, state_c))\n","\n","        loss = criterion(logits.transpose(1, 2), y)\n","\n","        state_h = state_h.detach()\n","        state_c = state_c.detach()\n","\n","        loss_value = loss.item()\n","\n","        loss.backward() # Perform back-propagation\n","\n","        # gradient clipping:\n","        _ = torch.nn.utils.clip_grad_norm_(\n","            model.parameters(), flags2.gradients_norm)\n","\n","        optimizer.step()\n","\n","        if iteration % 50 == 0:\n","            print(f'Epoch: {e}/{n_epochs} Iteration: {iteration} Loss: {loss_value}')\n","\n","        if iteration % 1000 == 0:\n","            predict(device, model, [flags2.initial_words], n_vocab, vocab_to_int, int_to_vocab)\n","\n","            torch.save(model.state_dict(),\n","                       f'/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/checkpoint_pt/model_ru-{iteration}.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wukM1tTYUbP2","executionInfo":{"status":"error","timestamp":1701414069308,"user_tz":-180,"elapsed":4123424,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"f9fd012f-cbad-41c9-d558-aed0367c0f39"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0/20 Iteration: 6050 Loss: 3.493364095687866\n","Epoch: 0/20 Iteration: 6100 Loss: 3.705981731414795\n","Epoch: 0/20 Iteration: 6150 Loss: 3.8550615310668945\n","Epoch: 0/20 Iteration: 6200 Loss: 4.063878536224365\n","Epoch: 0/20 Iteration: 6250 Loss: 4.2456254959106445\n","Epoch: 0/20 Iteration: 6300 Loss: 3.994547128677368\n","Epoch: 0/20 Iteration: 6350 Loss: 3.6236371994018555\n","Epoch: 0/20 Iteration: 6400 Loss: 4.201777935028076\n","Epoch: 0/20 Iteration: 6450 Loss: 5.041351318359375\n","Epoch: 0/20 Iteration: 6500 Loss: 4.99310302734375\n","Epoch: 0/20 Iteration: 6550 Loss: 5.000590801239014\n","Epoch: 1/20 Iteration: 6600 Loss: 3.295138120651245\n","Epoch: 1/20 Iteration: 6650 Loss: 2.9929749965667725\n","Epoch: 1/20 Iteration: 6700 Loss: 3.3319642543792725\n","Epoch: 1/20 Iteration: 6750 Loss: 3.2159202098846436\n","Epoch: 1/20 Iteration: 6800 Loss: 3.300675392150879\n","Epoch: 1/20 Iteration: 6850 Loss: 3.330460786819458\n","Epoch: 1/20 Iteration: 6900 Loss: 3.606283187866211\n","Epoch: 1/20 Iteration: 6950 Loss: 3.0831797122955322\n","Epoch: 1/20 Iteration: 7000 Loss: 3.841968297958374\n","я очень\n","\n","нравится, не имея в своем хозяйстве. привел последнего свидания – я боюсь, от этого обмана! когда степан аркадьич был с отцом, чтобы раздор вперед на все утро не имели намека на него многое, родные только, заболел оттого, чтоб в его грязном первым самой центрами конце земли, за обедом: и стали по\n","Epoch: 1/20 Iteration: 7050 Loss: 4.036503791809082\n","Epoch: 1/20 Iteration: 7100 Loss: 4.321037769317627\n","Epoch: 1/20 Iteration: 7150 Loss: 4.036653518676758\n","Epoch: 2/20 Iteration: 7200 Loss: 2.9292702674865723\n","Epoch: 2/20 Iteration: 7250 Loss: 3.127868175506592\n","Epoch: 2/20 Iteration: 7300 Loss: 2.908357858657837\n","Epoch: 2/20 Iteration: 7350 Loss: 2.9862425327301025\n","Epoch: 2/20 Iteration: 7400 Loss: 3.4482901096343994\n","Epoch: 2/20 Iteration: 7450 Loss: 3.08935284614563\n","Epoch: 2/20 Iteration: 7500 Loss: 3.1282694339752197\n","Epoch: 2/20 Iteration: 7550 Loss: 3.485750198364258\n","Epoch: 2/20 Iteration: 7600 Loss: 3.6684231758117676\n","Epoch: 2/20 Iteration: 7650 Loss: 3.775566816329956\n","Epoch: 2/20 Iteration: 7700 Loss: 3.552420139312744\n","Epoch: 3/20 Iteration: 7750 Loss: 2.9414970874786377\n","Epoch: 3/20 Iteration: 7800 Loss: 3.04245662689209\n","Epoch: 3/20 Iteration: 7850 Loss: 3.0653798580169678\n","Epoch: 3/20 Iteration: 7900 Loss: 2.9638216495513916\n","Epoch: 3/20 Iteration: 7950 Loss: 2.5631954669952393\n","Epoch: 3/20 Iteration: 8000 Loss: 2.8899660110473633\n","я очень\n","\n","умный, но он воображал ему. это и не правда?» а не хотелось петь. зачем он обвенчан, что девушке и ему все казалось, ни к его не мог от этого всего выйдет, он был нынче; кто на середине кити. и ему захотелось починить глаза. он выпрямил головой. и ему не надо ей\n","Epoch: 3/20 Iteration: 8050 Loss: 2.987464189529419\n","Epoch: 3/20 Iteration: 8100 Loss: 2.8982551097869873\n","Epoch: 3/20 Iteration: 8150 Loss: 3.173067569732666\n","Epoch: 3/20 Iteration: 8200 Loss: 3.15441632270813\n","Epoch: 3/20 Iteration: 8250 Loss: 3.424814224243164\n","Epoch: 3/20 Iteration: 8300 Loss: 3.408674955368042\n","Epoch: 4/20 Iteration: 8350 Loss: 2.7159125804901123\n","Epoch: 4/20 Iteration: 8400 Loss: 2.6968414783477783\n","Epoch: 4/20 Iteration: 8450 Loss: 2.9638094902038574\n","Epoch: 4/20 Iteration: 8500 Loss: 2.952486038208008\n","Epoch: 4/20 Iteration: 8550 Loss: 2.861236095428467\n","Epoch: 4/20 Iteration: 8600 Loss: 2.7254014015197754\n","Epoch: 4/20 Iteration: 8650 Loss: 2.7993505001068115\n","Epoch: 4/20 Iteration: 8700 Loss: 3.041557788848877\n","Epoch: 4/20 Iteration: 8750 Loss: 3.2027599811553955\n","Epoch: 4/20 Iteration: 8800 Loss: 3.07875394821167\n","Epoch: 4/20 Iteration: 8850 Loss: 3.11629319190979\n","Epoch: 5/20 Iteration: 8900 Loss: 2.945126533508301\n","Epoch: 5/20 Iteration: 8950 Loss: 2.6255717277526855\n","Epoch: 5/20 Iteration: 9000 Loss: 2.563282012939453\n","я очень\n","\n","люблю: снисходит о ней, – я нахожу, и я хочу сказать… вот вы клуб не хочет ли не проедете, константин дмитрич. не хочу. – сказал левин, задетый за ним. и не было сказано. но достойно и хотел подхватить это убедительное и бегавшая, как и я бы не мог сохранять свое положение.\n","Epoch: 5/20 Iteration: 9050 Loss: 3.110501766204834\n","Epoch: 5/20 Iteration: 9100 Loss: 2.6883738040924072\n","Epoch: 5/20 Iteration: 9150 Loss: 3.0708212852478027\n","Epoch: 5/20 Iteration: 9200 Loss: 2.7244441509246826\n","Epoch: 5/20 Iteration: 9250 Loss: 2.9392971992492676\n","Epoch: 5/20 Iteration: 9300 Loss: 2.864863395690918\n","Epoch: 5/20 Iteration: 9350 Loss: 2.9106240272521973\n","Epoch: 5/20 Iteration: 9400 Loss: 3.1820743083953857\n","Epoch: 5/20 Iteration: 9450 Loss: 2.984186887741089\n","Epoch: 6/20 Iteration: 9500 Loss: 2.6874265670776367\n","Epoch: 6/20 Iteration: 9550 Loss: 2.5903735160827637\n","Epoch: 6/20 Iteration: 9600 Loss: 2.725654125213623\n","Epoch: 6/20 Iteration: 9650 Loss: 2.825080394744873\n","Epoch: 6/20 Iteration: 9700 Loss: 3.0392532348632812\n","Epoch: 6/20 Iteration: 9750 Loss: 2.8113389015197754\n","Epoch: 6/20 Iteration: 9800 Loss: 3.2154037952423096\n","Epoch: 6/20 Iteration: 9850 Loss: 2.7190213203430176\n","Epoch: 6/20 Iteration: 9900 Loss: 3.1733129024505615\n","Epoch: 6/20 Iteration: 9950 Loss: 3.074859857559204\n","Epoch: 6/20 Iteration: 10000 Loss: 2.702484607696533\n","я очень\n","\n","рад, что это было просто мама. но это так не хотите сказать мне все оказать словам. так покойно письменного маленькие воде, левина, как во всем, если это будет происходить из того, что она была уверена, что она не только ждала, она была жестокая насмешка вчера, или переезжать много книг оттого сколько\n","Epoch: 7/20 Iteration: 10050 Loss: 2.614988327026367\n","Epoch: 7/20 Iteration: 10100 Loss: 2.444720506668091\n","Epoch: 7/20 Iteration: 10150 Loss: 2.9051079750061035\n","Epoch: 7/20 Iteration: 10200 Loss: 3.013535261154175\n","Epoch: 7/20 Iteration: 10250 Loss: 2.7561469078063965\n","Epoch: 7/20 Iteration: 10300 Loss: 2.8194262981414795\n","Epoch: 7/20 Iteration: 10350 Loss: 2.585463285446167\n","Epoch: 7/20 Iteration: 10400 Loss: 2.746492624282837\n","Epoch: 7/20 Iteration: 10450 Loss: 2.7081308364868164\n","Epoch: 7/20 Iteration: 10500 Loss: 2.7531075477600098\n","Epoch: 7/20 Iteration: 10550 Loss: 3.0334575176239014\n","Epoch: 7/20 Iteration: 10600 Loss: 2.8360424041748047\n","Epoch: 8/20 Iteration: 10650 Loss: 2.7806639671325684\n","Epoch: 8/20 Iteration: 10700 Loss: 2.8222496509552\n","Epoch: 8/20 Iteration: 10750 Loss: 2.4527783393859863\n","Epoch: 8/20 Iteration: 10800 Loss: 2.8143579959869385\n","Epoch: 8/20 Iteration: 10850 Loss: 2.8053574562072754\n","Epoch: 8/20 Iteration: 10900 Loss: 2.663590431213379\n","Epoch: 8/20 Iteration: 10950 Loss: 2.589217185974121\n","Epoch: 8/20 Iteration: 11000 Loss: 2.832530975341797\n","я очень\n","\n","рад. – сказал степан аркадьич и весел, чтобы приезд в этом положении. пренебрегают молчания, той, кто у ней. – а я вам тебе говорю, чтоб ей стыдно – да я вас расстроил. меня. приятного за шафером, участие, несмотря в голосе, во-первых, – прохрипел он удивился; закрыв руку на господина, который, улыбаясь\n","Epoch: 8/20 Iteration: 11050 Loss: 2.7236366271972656\n","Epoch: 8/20 Iteration: 11100 Loss: 2.7424795627593994\n","Epoch: 8/20 Iteration: 11150 Loss: 2.677716016769409\n","Epoch: 9/20 Iteration: 11200 Loss: 2.549562692642212\n","Epoch: 9/20 Iteration: 11250 Loss: 2.8503761291503906\n","Epoch: 9/20 Iteration: 11300 Loss: 2.778860569000244\n","Epoch: 9/20 Iteration: 11350 Loss: 2.598173141479492\n","Epoch: 9/20 Iteration: 11400 Loss: 2.684027910232544\n","Epoch: 9/20 Iteration: 11450 Loss: 2.8283073902130127\n","Epoch: 9/20 Iteration: 11500 Loss: 2.5284736156463623\n","Epoch: 9/20 Iteration: 11550 Loss: 2.5589582920074463\n","Epoch: 9/20 Iteration: 11600 Loss: 2.337794065475464\n","Epoch: 9/20 Iteration: 11650 Loss: 2.6404855251312256\n","Epoch: 9/20 Iteration: 11700 Loss: 2.7030646800994873\n","Epoch: 9/20 Iteration: 11750 Loss: 2.6769967079162598\n","Epoch: 10/20 Iteration: 11800 Loss: 2.720184803009033\n","Epoch: 10/20 Iteration: 11850 Loss: 2.723475456237793\n","Epoch: 10/20 Iteration: 11900 Loss: 2.942863941192627\n","Epoch: 10/20 Iteration: 11950 Loss: 2.935983180999756\n","Epoch: 10/20 Iteration: 12000 Loss: 2.840939998626709\n","я очень\n","\n","рад был, как умела. это и не была не только близка ушла на себя не судья, всегда заботы. а он не мог вынести в упорстве традиционности, прием опиума или сюртука, а у нее католического священника, раздумал и вопросительно посмотрела рядом с радостною улыбкой покачал головой, чтоб убедить по крахмаленым своими мыслями,\n","Epoch: 10/20 Iteration: 12050 Loss: 2.6998164653778076\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-ac2b8fed1f0c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Perform back-propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# gradient clipping:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(logits.transpose(1, 2)[0].shape, y[0].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcrX3GLvdIAY","executionInfo":{"status":"ok","timestamp":1700899782961,"user_tz":-180,"elapsed":8,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"d7a26da7-d369-4c22-bbed-85b6cfa88202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([53301, 32]) torch.Size([32])\n"]}]},{"cell_type":"code","source":["print(logits.transpose(1, 2)[0, 0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeD1HhhBdUbz","executionInfo":{"status":"ok","timestamp":1700899675886,"user_tz":-180,"elapsed":8,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"68ffcdc3-7136-49e0-9bae-5e6f47cbcda0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-3.9834, -5.4743, -4.1930, -4.5456, -6.2130, -5.5867, -6.6796, -4.2893,\n","        -3.9535, -3.0015, -5.0683, -3.1591, -4.4779, -2.1410, -3.1516, -3.7769,\n","        -3.6955, -2.4978, -5.4208, -6.8305, -7.6931, -3.6254, -6.8509, -5.9198,\n","        -3.5860, -2.4918, -2.1399, -6.2458, -8.5704, -3.4032, -1.6102, -9.4341],\n","       grad_fn=<SelectBackward0>)\n"]}]},{"cell_type":"code","source":["print(logits[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFZROfkYeDqT","executionInfo":{"status":"ok","timestamp":1700899669960,"user_tz":-180,"elapsed":12,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"ae1975e5-c57f-407d-cbd9-128d88d69327"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ -3.9834, -10.9878,  -5.0249,  ..., -17.8426, -17.7655, -17.8216],\n","        [ -5.4743,  -4.7344,  -4.4455,  ..., -24.1769, -24.7833, -24.4461],\n","        [ -4.1930,  -3.5709,  -4.1695,  ..., -15.3681, -15.6860, -15.5729],\n","        ...,\n","        [ -3.4032,  -7.6013, -12.5809,  ..., -29.7955, -30.3856, -29.8458],\n","        [ -1.6102,  -4.7668,  -1.5558,  ..., -18.9417, -19.5016, -19.0712],\n","        [ -9.4341,  -8.5210,  -8.1673,  ..., -19.7130, -20.1074, -19.7240]],\n","       grad_fn=<SelectBackward0>)\n"]}]},{"cell_type":"code","source":["y[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ythYaBwqh70i","executionInfo":{"status":"ok","timestamp":1700899836161,"user_tz":-180,"elapsed":8,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"ac2054e6-fe7f-4113-d6ca-aca85b5447a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 1509,     3,   155,     0,    27,  2661,     4, 21380,     1,  2206,\n","          233,    11,     8,     2,   933,   468,     1,    19,   105,   907,\n","        12235,     0,   123,    23,     5,   769,    20,  3750, 12236,     0,\n","           20,    61])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n","    flags.text, flags.batch_size, flags.seq_size)\n","\n","iteration = 9600\n","\n","model = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n","model = model.to(device)\n","model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Colab Notebooks/тексты/4_LSTM/checkpoint_pt/model-{iteration}.pth\"))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n","\n","predict(device, model, [['help', 'me']], n_vocab, vocab_to_int, int_to_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BpNi4V4eM9U","executionInfo":{"status":"ok","timestamp":1700899072913,"user_tz":-180,"elapsed":1187,"user":{"displayName":"Ekaterina Esina","userId":"08559422542032477941"}},"outputId":"0548a846-2386-4ca5-9e19-815185d3b918"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size  17682\n","num_batches: 193\n","help me\n","\n","herself, and so doing, the same as an comprehensiveness would we clearly in the drawer with which we said, for as galiani still survives. after the entire modern men, of their masters and revolt at the last century, when precisely this worthy love, be courses of thought, they set men and\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"A7X_iNkDfj51"},"execution_count":null,"outputs":[]}]}